{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [logistic regression](https://github.com/johnnychiuchiu/Machine-Learning/blob/master/LogisticRegression/logisticRegression.pdf), we models the probability of our response y belongs to a particular category by building a linear model of x on the log-odds of y. In discriminant analysis, instead of estimating this probability, we calculate the probability of each observation belongs to each category. It is indeed similar to [Gaussian Mixture Model](https://github.com/johnnychiuchiu/Machine-Learning/blob/master/Clustering/GaussianMixtureModel/gmm.ipynb), the difference is that in GMM we don't have the category in the response variable, but in discriminant analysis we do have it. The difference make the whole problem become more straight forward, since we don't need to use EM step to estimate the mean and variance of each group. Instead, we can use the mean and variance of each group to calculate the probability that a certain observation belongs to each cluster k.\n",
    "\n",
    "Something we need to keep in mind is that discriminant analysis do not necessary use Gaussian to calculate how likelihood a certain observation belongs to different category. In the simplest form, we can calculate this likelihood (*not to be confused with maximum likelihood, the likelihood here does not related to MLE*) by calculate the Euclidean distance between the observation to the mean of each group. Also, the predictors in discriminant analysis are required to be numerical, since we cannot calculate the distance of categorical variables.\n",
    "\n",
    "In the following paragraph, we will discuss different methods to do classification using discriminant analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Discriminant Function from Mahalnobis Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously discussed, we can simply use Euclidean distance(L2-Norm) to decide the new observation should be in which group by calculating its distance to each group. However, it will have two problems.\n",
    "1. it combines variables with different units and scales of measurement, e.g. we are adding apples and oranges.\n",
    "2. It doesnot take into account the different variances of the variables. For example, the variables with large variances should be weighted less; \n",
    "3. It doesnot take into account the different correlations among variables. If two variables are highly correlated them both should not be highly weighted.\n",
    "\n",
    "We can address the second problem by dividing the standard deviation of each variable for calculating Eculidean distance. However, the third problem is still there.\n",
    "\n",
    "Therefore, we should need a method that help us deal with the correlation problem when calculating distance. The distance we use to address this problem is called **Mahalnobis Distance**, which is defined as follows according to wikipedia:\n",
    "\n",
    "The Mahalanobis distance of an observation $\\displaystyle {\\vec {x}}=(x_{1},x_{2},x_{3},\\dots ,x_{N})^{T}$ from a set of observations with mean $\\displaystyle {\\vec {\\mu }}=(\\mu _{1},\\mu _{2},\\mu _{3},\\dots ,\\mu _{N})^{T}$ and covariance matrix S is defined as:\n",
    "\n",
    "$$\\displaystyle d_{M}^2({\\vec {x}})={({\\vec {x}}-{\\vec {\\mu }})^{T}S^{-1}({\\vec {x}}-{\\vec {\\mu }})}$$\n",
    "\n",
    "\n",
    "The Mahalanobis distance has taken into account variances and covariances of the variables. Therefore, when the covariances is high, the weight of both variable will be lower calculated from this distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we denote $\\vec{x}$ as $x$, the Linear Discriminant Function derived from Mahalnobis distance is \n",
    "\n",
    "$$L_i = \\bar{x_i}'S^{-1}x - \\frac{1}{2}\\bar{x_i}'S^{-1}\\bar{x_i}$$\n",
    "\n",
    "We classify new observation into group $i$ where $L_i$ is maximum.\n",
    "If we have two predictor($x_1, x_2$), and two reponse categories($1, 2$), then in the LDF from Mahalnobis, we will have 6 coefficients. For example, the LDF can be\n",
    "$$L_1 = \\frac{7}{4}x_1+\\frac{2}{3}x_2-\\frac{65}{8}$$\n",
    "$$L_2 = x_1+\\frac{1}{3}x_2-\\frac{5}{2}$$\n",
    "\n",
    "To sum up, we first use Mahalnobis Distance that take variances and covariances into account to calculate our ideal distance, and then we choose the group that has smaller distance to our new observation as the classified group, that is if $d_1 < d_2$, then we should classify $x$ in to group 1. The inequality $d_1 < d_2$ equals $L_1 > L_2$. Therefore, we can also use $L_i$ to get the result. It is a linear function of x, since we assume the covariance of each group is the same. Therefore, we deriving L_i from $d_1 < d_2$, the quadratic term can be deleted from both side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fisher's Linear Discriminant Function (LDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the classification rule for the two groups case depends only on a single difference $L_1-L_2$, it can be reformulated in terms of a single linear function of $x$ given by\n",
    "\n",
    "$$Fisher's LDF = LD(x) = (\\bar{x_1}-\\bar{x_1})'S^{-1}x$$\n",
    "\n",
    "It can be think of a projection function, that is, the function project $x$ on the straight line defined by this function. We decide whether a new observation $x$ should be classify into group 1 or 2 by comparing the distance of $LD(x)$ to $LD(\\bar{x_1})$ and $LD(x)$ to $LD(\\bar{x_2})$. If the previous is smaller, then we say the new observation should be classify as group 1. Note that the distance we use to calculate the $LD(x)$ to $LD(\\bar{x_i})$ is Eculidean Distance, because it represent the distance of point on this straight line.\n",
    "\n",
    "For example, a Fisher's LDF can be\n",
    "$$LD= \\frac{2}{3}x_1+\\frac{1}{9}x_2$$\n",
    "\n",
    "We then use the same function using new observation, the mean vector of $x_1$ and $x_2$. Then by comparing the distance between $\\big(LD(new observation),LD(x_1)\\big)$ and $\\big(LD(new observation),LD(x_2)\\big)$, whichever is smaller, we classify new observation in that group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Bayes' Theorem for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From *Bayes Formula*, we know that \n",
    "\n",
    "$$Pr(Y=k|X=x) = \\frac{\\pi_k f_k(x)}{\\sum_{j=1}^{k}\\pi_j f_j(x)}$$\n",
    "\n",
    "where $\\pi_j$ is the posterior probability of our response.\n",
    "If we assume $f_i{x}$ is a multivariate normal distribution, then after some derivation by assuming all the m groups have the same covariance, we can get:\n",
    "\n",
    "$$Pr(Y=k|X=x) = \\frac{\\pi_k e^{L_k}}{\\sum_{j=1}^{k}\\pi_j e^{L_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *lda function of MASS* library in r, it has a parameter called `prior`. It is the parameter we use to specify the prior probability for each group in our response variable. If unspecified, the class proportions for the training set are used. If present, the probabilities should be specified in the order of the factor levels.\n",
    "\n",
    "If the prior probabilties are equal, then the Bayes classification rule reduces to the same equation as minimum Mahalbonis distance or maximum discrimanint score. \n",
    "\n",
    "We can also think of LDF as a classifier that get results from assuming that the observations from each class are drawn from a Gaussian distribution with the same covariance matrix, and plugging estimates for the parameters into Bates' theorem in order to preform prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • An LDA example in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(MASS)\n",
    "data(iris)\n",
    "iris$Species = as.character(iris$Species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make the dataframe into only two classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_two_class = iris %>% filter(Species %in% c('setosa','versicolor'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit a Linear discriminant analysis model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_two_class = lda(Species ~ . , data=iris_two_class, prior=c(0.5,0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the fitted result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "lda(Species ~ ., data = iris_two_class, prior = c(0.5, 0.5))\n",
       "\n",
       "Prior probabilities of groups:\n",
       "    setosa versicolor \n",
       "       0.5        0.5 \n",
       "\n",
       "Group means:\n",
       "           Sepal.Length Sepal.Width Petal.Length Petal.Width\n",
       "setosa            5.006       3.428        1.462       0.246\n",
       "versicolor        5.936       2.770        4.260       1.326\n",
       "\n",
       "Coefficients of linear discriminants:\n",
       "                   LD1\n",
       "Sepal.Length -0.300458\n",
       "Sepal.Width  -1.773845\n",
       "Petal.Length  2.142260\n",
       "Petal.Width   3.035726"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_two_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict new observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• ***Calculate manually***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *MASS* package calculates Fisher's LDF. The coefficient shown above is the coefficient for Fisher's LDF. If we have a new observation (5, 3, 2, 0.5), then we can classify it into either group by calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] -1.021443\n"
     ]
    }
   ],
   "source": [
    "new_obs = data.frame(Sepal.Length=5, Sepal.Width=3, Petal.Length=2, Petal.Width=0.5)\n",
    "LD_new_observation = sum(fit_two_class$scaling * new_obs)\n",
    "print(LD_new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] -3.706061\n",
      "[1] 6.45433\n"
     ]
    }
   ],
   "source": [
    "LD_setosa = sum(fit_two_class$scaling*fit_two_class$means[1,])\n",
    "LD_versicolor = sum(fit_two_class$scaling*fit_two_class$means[2,])\n",
    "print(LD_setosa)\n",
    "print(LD_versicolor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 7.207177\n",
      "[1] 55.88717\n"
     ]
    }
   ],
   "source": [
    "distance_setosa = (LD_new_observation-LD_setosa)^2\n",
    "distance_versicolor = (LD_new_observation-LD_versicolor)^2\n",
    "print(distance_setosa)\n",
    "print(distance_versicolor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between LD(x) to setosa is smaller than versicolor, so the new observation should be classify as Sentosa. We can also use library to get the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• ***Calculate using library***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$class</dt>\n",
       "\t\t<dd>setosa</dd>\n",
       "\t<dt>$posterior</dt>\n",
       "\t\t<dd><table>\n",
       "<thead><tr><th></th><th scope=col>setosa</th><th scope=col>versicolor</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1           </td><td>2.687037e-11</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</dd>\n",
       "\t<dt>$x</dt>\n",
       "\t\t<dd><table>\n",
       "<thead><tr><th></th><th scope=col>LD1</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>-2.395577</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$class] setosa\n",
       "\\item[\\$posterior] \\begin{tabular}{r|ll}\n",
       "  & setosa & versicolor\\\\\n",
       "\\hline\n",
       "\t1 & 1            & 2.687037e-11\\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item[\\$x] \\begin{tabular}{r|l}\n",
       "  & LD1\\\\\n",
       "\\hline\n",
       "\t1 & -2.395577\\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$class\n",
       ":   setosa\n",
       "$posterior\n",
       ":   \n",
       "| <!--/--> | setosa | versicolor | \n",
       "|---|\n",
       "| 1 | 1            | 2.687037e-11 | \n",
       "\n",
       "\n",
       "\n",
       "$x\n",
       ":   \n",
       "| <!--/--> | LD1 | \n",
       "|---|\n",
       "| 1 | -2.395577 | \n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$class\n",
       "[1] setosa\n",
       "Levels: setosa versicolor\n",
       "\n",
       "$posterior\n",
       "  setosa   versicolor\n",
       "1      1 2.687037e-11\n",
       "\n",
       "$x\n",
       "        LD1\n",
       "1 -2.395577\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(fit_two_class, newdata=new_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the posterior probability for *Sentosa* is much higher than *Versicolor*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis (QDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have discussed, LDA assume that the covariance matrix is common to all $K$ classes. If the homoscedasticity assumption is dropped, then the quadratic term cannot be deleted when deriving the discriminant function. Therefore, suppose we have two classes, and denote that the distance of a datapoint to group1 and group2 is $d_1$ and $d_2$, then the inqeuality $d_1^2 < d_2^2$ becomes $Q_1 > Q_2$, where \n",
    "$$Q_i = -\\frac{1}{2}x'S_i^{-1}x+\\bar{x_i}'S_i^{-1}x-\\frac{1}{2}\\bar{x_i}'S_i^{-1}\\bar{x_i}$$\n",
    "\n",
    "It is a quadratic function of $x$, and we can use this function to form the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_two_class_qda = qda(Species ~ . , data=iris_two_class, prior=c(0.5,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "qda(Species ~ ., data = iris_two_class, prior = c(0.5, 0.5))\n",
       "\n",
       "Prior probabilities of groups:\n",
       "    setosa versicolor \n",
       "       0.5        0.5 \n",
       "\n",
       "Group means:\n",
       "           Sepal.Length Sepal.Width Petal.Length Petal.Width\n",
       "setosa            5.006       3.428        1.462       0.246\n",
       "versicolor        5.936       2.770        4.260       1.326"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_two_class_qda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of qda contains the group means, But it does not contain the coefficients of hte linear discriminants, because hte QDA classifier involves a quadratic, rather than a linear function of the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Comparision of Classification Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When the classes are well-separated, then LDA or QDA will perform better, since the parameters estimates for the logistics regression using MLE, which will be unstable in this case\n",
    "* LDA assumes that the observations are drawn from a Gaussian distribution with a common covariance matrix in each class, and so can provide some imporvements over logistic regression when this assumption holds. Conversely, logistic regression can outperform LDA if there Gaussian assumptions are not met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "* Predictive Analytics: Paramertic Models for Regression and Classificantion by Ajit C. Tamhane and Edward C. Malthouse\n",
    "* [An Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "* [Mahalanobis Distance Wikipedia](https://en.wikipedia.org/wiki/Mahalanobis_distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
